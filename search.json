[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "remote-sensing-diary",
    "section": "",
    "text": "Introduction\nWith an academic background in urban and rural planning, my prior studies and research have primarily centered around design thinking and conceptual frameworks. While I’ve had some experience with GIS, this module marks my first formal engagement with remote sensing and Earth observation technologies. As such, it feels both refreshing and intellectually stimulating to explore cities not just through spatial design, but through the lens of satellite data and spectral analysis.\nRemote sensing introduces an entirely new dimension to how we understand urban dynamics. Concepts like land surface temperature, vegetation indices (e.g., NDVI), and temporal land cover change offer powerful, objective tools to assess urban conditions over time and space. This data-driven perspective is especially valuable in complementing the more qualitative, human-centered methods I’ve worked with in the past.\nI’ve always had a strong passion for geography—something that has shaped the way I look at the world since childhood. Now, being able to merge that innate curiosity with technical skills in GIS and remote sensing opens up exciting possibilities. Through this module and the broader degree program, I hope to strengthen my ability to integrate geospatial data into urban planning practices, and to conduct more analytical, evidence-based research. Ultimately, I aim to develop a more holistic planning approach—one that bridges design intuition with measurable environmental realities.",
    "crumbs": [
      "Home",
      "remote-sensing-diary"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "Chapter 1: Getting Started with Remote Sensing",
    "section": "",
    "text": "Remote sensing involves acquiring information about Earth’s surface through the measurement and analysis of electromagnetic radiation emitted or reflected from the surface without direct contact. Central to this process is the electromagnetic spectrum, encompassing wavelengths from ultraviolet, visible, and infrared to microwave, each interacting differently with atmospheric constituents and terrestrial surfaces. Upon encountering Earth’s atmosphere, electromagnetic radiation undergoes scattering processes such as Rayleigh, Mie, and non-selective scattering, affecting both data quality and interpretation. Understanding these scattering mechanisms is critical for effective atmospheric correction and accurate surface interpretation. \n\nFig 1.1 Source: Julien Chimot, from Bovensmann et al., 2011\n\nOnce radiation reaches Earth’s surface, it interacts through reflection, absorption, and transmission. These interactions produce unique spectral signatures that enable identification of various land-cover types such as vegetation, soil, water bodies, and urban features. To effectively capture and utilize these interactions, remote sensing sensors possess four critical types of resolutions: spectral, spatial, temporal, and radiometric. Spectral resolution allows differentiation between wavelengths, aiding in distinguishing materials based on their spectral signatures. Spatial resolution determines the detail level or smallest discernible feature size. Temporal resolution defines the revisit frequency, crucial for monitoring dynamic changes, while radiometric resolution refers to the sensor’s sensitivity in detecting small differences in emitted or reflected radiation, influencing image detail and precision. \n\nFig 1.2 Source: NASA Science\n\nPlatforms like Landsat and Sentinel satellites provide global datasets balancing these resolutions, supporting extensive research in environmental monitoring, agriculture, forestry, and urban planning. Landsat, offering moderate-resolution imagery, facilitates long-term environmental studies, while Sentinel missions provide higher spatial and temporal resolution data, significantly improving precision in detailed land-use applications. Sentinel data are accessed primarily through the Copernicus Browser, a user-friendly platform offering efficient data retrieval and analysis, enhancing global data accessibility for diverse research needs. \n\nFig 1.3 Source: https://andrewmaclachlan.github.io/CASA0023/intro.html",
    "crumbs": [
      "Home",
      "Chapters",
      "Chapter 1: Getting Started with Remote Sensing"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "Chapter 3: Remote Sensing Data & Corrections",
    "section": "",
    "text": "Remote sensing imagery often requires comprehensive preprocessing to ensure accurate interpretation and reliable application. This preprocessing typically includes geometric correction, atmospheric correction, empirical line correction, orthorectification, and radiometric correction, which collectively ensure high-quality remote sensing data for precise analysis.\nGeometric correction addresses spatial distortions arising from sensor orientation and Earth’s curvature, aligning satellite imagery accurately with real-world geographic coordinates. Orthorectification specifically corrects imagery by considering terrain elevation variations, essential for accurate spatial alignment and mapping. \n\nFig 1.3 Source:Abdul Basith\n\nAtmospheric correction removes distortions caused by atmospheric particles and gases, ensuring that the spectral reflectance recorded represents true surface characteristics. Methods such as empirical line correction provide practical atmospheric correction approaches, calibrating remotely sensed images using known reflectance targets within the imagery, offering simplicity and efficiency, particularly for imagery lacking detailed atmospheric data.\nRadiometric correction adjusts sensor-measured radiance into physically meaningful surface reflectance values, correcting for sensor variations and illumination differences between images. The Landsat Analysis Ready Data (ARD) products exemplify rigorous radiometric correction by providing standardized surface reflectance datasets, significantly simplifying analytical workflows and promoting consistency across temporal and spatial scales. \n\nFig 1.2 Source: LaSRC (Landsat 8-9) Collection 2, level 2 product: https://www.usgs.gov/landsat-missions/landsat-collection-2-surface-reflectance\n\nFurthermore, image enhancement techniques, including histogram equalization, contrast stretching, and spatial filtering, improve visual interpretability of remotely sensed imagery, aiding subsequent classification, change detection, and other image-based analyses. Data joining methods facilitate combining imagery datasets across different dates, sensors, or ancillary data sources, enriching analytical contexts and enhancing interpretability, especially in multi-temporal or multi-sensor remote sensing analyses. Together, these preprocessing techniques create robust, high-quality data foundational for accurate environmental assessment, spatial analysis, and informed decision-making.\n# Option 1: Use custom study area shapefile\nstudy_area &lt;- st_read(\"prac_3/study_area.shp\") %&gt;%\n  st_transform(32634)\n\n# Clip and mask the raster (e.g., Sentinel-1) using the study area\nm1_clip &lt;- m1 %&gt;%\n  terra::crop(study_area) %&gt;%\n  terra::mask(study_area)\nYou can also use administrative boundaries from GADM:\n# Option 2: Use GADM Level 4 boundaries\nSA &lt;- st_read(\"prac_3/GADM/gadm41_ZAF.gpkg\", \n              layer = \"ADM_ADM_4\")\n\n# Filter to Cape Town and reproject\ncape_town &lt;- SA %&gt;%\n  filter(GID_4 == \"ZAF.9.3.1.87_1\") %&gt;%\n  st_transform(32634)\n\n# Clip and mask using Cape Town boundary\nm1_clip &lt;- m1 %&gt;%\n  terra::crop(cape_town) %&gt;%\n  terra::mask(cape_town)\n\nCode 1.1 Source: https://andrewmaclachlan.github.io/CASA0023/3_corrections.html\n\n\n\nFig 1.3 Source: Land use/cover classification in the Brazilian Amazon using satellite images",
    "crumbs": [
      "Home",
      "Chapters",
      "Chapter 3: Remote Sensing Data & Corrections"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "Chapter 4: Policy - Jakarta Flood Management and Mangrove Restoration",
    "section": "",
    "text": "Jakarta, Indonesia’s capital city, is increasingly vulnerable to severe flooding due to heavy rainfall, rapid urbanization, land subsidence, climate change, and inadequate drainage infrastructure. Frequent flooding events have exacerbated environmental degradation, particularly impacting mangrove wetlands—critical natural barriers protecting urban areas. Mangrove degradation not only reduces biodiversity but also weakens the city’s resilience against future floods. Flood-driven land degradation, combined with ongoing land-use transformations and climatic variability, presents significant environmental policy challenges. Effective management and restoration of mangrove ecosystems thus become essential components of sustainable urban strategies. Addressing these interconnected issues requires comprehensive monitoring approaches to inform targeted interventions, policy-making, and strategic urban development. \n\nFig 1.1 Map of Research Location\n\nJakarta, as part of the C40 Cities Climate Leadership Group, has implemented multiple policies to mitigate flooding. These policies include the Jakarta Coastal Defence Strategy and Flood Mapping initiative, limiting groundwater extraction, providing alternative water supplies, and developing water retention basins. The city also launched the Socially Inclusive Climate Adaptation for Urban Revitalization Project, relocating residents from flood-prone areas and revitalizing reservoirs to enhance water storage. Additionally, Jakarta aims to increase green open spaces from 10% to 30% by 2030, constructing parks to reduce flood duration and improve life quality. \n\nFig 1.2 Source: https://www.c40.org/networks/urban-flooding-network/",
    "crumbs": [
      "Home",
      "Chapters",
      "Chapter 4: Policy - Jakarta Flood Management and Mangrove Restoration"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "Chapter 2: Xaringan Presentation and Sentinel-2 Remote Sensing Data",
    "section": "",
    "text": "The practical application of Xaringan presentations can be demonstrated through a detailed exploration of Sentinel-2 data applications. I have developed an interactive presentation illustrating vegetation indices such as NDVI and EVI calculated from Sentinel-2 data, highlighting their utility in environmental monitoring and precision agriculture. 👉 My Xaringan Sentinel-2 Remote Sensing Presentation\n\n \n\n\nReflection\nUsing Xaringan for presentations significantly enhanced my ability to communicate complex remote sensing analyses clearly and interactively. Initially, mastering its syntax posed challenges, but Xaringan’s integration with R Markdown simplified incorporating interactive graphs, maps, and code outputs, streamlining the presentation creation process. I appreciated the flexibility Xaringan offered, allowing dynamic and visually appealing presentations directly from data analysis workflows.For future work, I envision using Xaringan extensively for stakeholder engagement and academic conferences, showcasing detailed remote sensing results more effectively.",
    "crumbs": [
      "Home",
      "Chapters",
      "Chapter 2: Xaringan Presentation and Sentinel-2 Remote Sensing Data"
    ]
  },
  {
    "objectID": "chapter8.html",
    "href": "chapter8.html",
    "title": "Chapter 8: Synthetic Aperture Radar (SAR) and SAR-Optical Data Fusion",
    "section": "",
    "text": "Synthetic Aperture Radar (SAR) technology has transformed remote sensing by providing reliable, all-weather, and day-night Earth observation capabilities. Unlike traditional optical sensors, which are limited by atmospheric conditions, SAR employs active microwave signals capable of penetrating clouds to consistently capture high-resolution imagery. Interferometric SAR (InSAR) enhances SAR capabilities by leveraging phase differences between SAR images acquired at different times, accurately measuring subtle Earth’s surface elevation changes. Differential InSAR (DInSAR) specifically measures surface displacement with millimeter-level accuracy, significantly aiding disaster prediction and environmental management. Nevertheless, these techniques encounter limitations such as atmospheric disturbances, phase decorrelation, and complex phase unwrapping procedures, impacting their operational feasibility. \n\nFig 1.1 InSAR. Source:GeoScience Australia\n\nReceiver Operating Characteristic (ROC) curves have increasingly become essential for evaluating SAR and InSAR classification and detection methods. ROC curves graphically illustrate the trade-off between sensitivity (true positive rate) and false alarm rates, enabling optimization of classification thresholds and objective performance assessment, thus enhancing detection confidence. \n\nFig 1.2 Source:MLU-EXPLAIN\n\nAdvancements now involve integrating SAR with optical remote sensing data, leveraging complementary advantages to overcome individual sensor limitations. Fusion methods such as Principal Component Analysis (PCA), Object-Based Image Analysis (OBIA), and Intensity Fusion enhance data interpretability and accuracy. PCA reduces data redundancy by transforming multiple input bands from SAR and optical imagery into fewer, more informative principal components. OBIA segments imagery into meaningful geographic objects, effectively combining structural detail from SAR with rich spectral information from optical data. Intensity Fusion integrates SAR’s spatial detail with optical spectral fidelity, significantly improving land cover classification and urban planning applications. Despite clear benefits, data fusion must manage challenges related to spatial-temporal alignment, computational complexity, and sensor compatibility. \n\nFig 1.3 Image fusion result of SAR and optical imagery. Source: ASF has useful SAR guide",
    "crumbs": [
      "Home",
      "Chapters",
      "Chapter 8: Synthetic Aperture Radar (SAR) and SAR-Optical Data Fusion"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "Chapter 6: Classification I - Remote Sensing Image Classification Techniques",
    "section": "",
    "text": "Classification of remote sensing imagery has greatly evolved with advancements in machine learning techniques, significantly improving the extraction of land-cover information from complex datasets. Supervised classification approaches rely on labeled training data to guide model learning, typically delivering higher classification accuracy in clearly defined scenarios. Conversely, unsupervised methods cluster imagery based solely on intrinsic spectral and spatial properties, providing initial insights and assisting in discovering unknown land-cover categories. Although unsupervised classification efficiently handles exploratory tasks, it may produce ambiguous classifications and lacks interpretability. Supervised approaches, though accurate, heavily depend on extensive, high-quality training samples challenging to collect practically.\nWithin supervised classification, regression trees and random forests have become particularly effective due to their robustness, interpretability, and predictive power. Regression trees partition data recursively into homogeneous subsets using straightforward decision rules, making them highly interpretable yet prone to overfitting. Random forests overcome this limitation using an ensemble of regression trees trained on bootstrapped samples and random subsets of variables. Aggregating predictions from multiple trees significantly reduces model variance and mitigates overfitting, enhancing generalization performance. \n\nFig 1.1 Source:Rosaria Silipo\n\nSupport Vector Machines (SVM) represent another powerful supervised classification method, particularly suitable for remote sensing data characterized by high-dimensional feature spaces. SVM identifies an optimal decision boundary (hyperplane) maximizing the margin between classes, providing high accuracy even with limited labeled training data. However, successful SVM applications depend heavily on kernel selection and parameter optimization, requiring substantial computational resources and expertise. \n\nFig 1.2 Source:Drew Wilimitis\n\nEffective classification accuracy assessments typically employ error matrices and metrics such as Producer’s Accuracy (PA), User’s Accuracy (UA), Overall Accuracy (OA), and Kappa statistics, collectively providing quantitative measures of classification reliability.",
    "crumbs": [
      "Home",
      "Chapters",
      "Chapter 6: Classification I - Remote Sensing Image Classification Techniques"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "Chapter 5: An Introduction to Google Earth Engine",
    "section": "",
    "text": "Google Earth Engine (GEE) is a cloud-based platform widely used for efficiently processing large-scale geospatial datasets. Its robust infrastructure allows users to access, analyze, and visualize extensive satellite imagery collections without requiring local computational resources. The initial setup of GEE is straightforward, involving account registration, accessing the code editor, and familiarizing oneself with JavaScript or Python interfaces for scripting. GEE’s key strengths lie in its diverse built-in functions and tools, including loading image collections, reducing images across specified spatial or temporal domains, performing regression analyses, and utilizing data joining and filtering techniques. \n\nFig 1.1 Source: GEE screen: GEE community Beginner’s Cookbook, TC25\n\nImage collections in GEE, such as Landsat, Sentinel, or MODIS data, are efficiently accessed using ee.ImageCollection(). Reducing image collections with functions like ee.Reducer.mean() allows users to generate representative images by computing statistical summaries (mean, median, sum) across multiple images, facilitating time-series analysis or regional studies. Regression analysis functions (ee.Reducer.linearFit()) facilitate modeling environmental trends, predicting continuous spatial variables, or evaluating relationships between imagery-derived indices and ground data. Additionally, filtering and joining datasets based on attributes (dates, sensor types, or spatial coverage) provide flexible, powerful methods for selecting relevant subsets from large datasets. \n\nFig 1.2 Source:https://andrewmaclachlan.github.io/CASA0023-lecture-5/#43\n\nAccuracy assessment and validation of GEE-derived results involve spatial cross-validation and statistical measures (e.g., Kappa statistics, Overall Accuracy), ensuring reliability and robustness. Despite numerous advantages, users initially face learning curves regarding scripting proficiency and understanding the cloud-based execution paradigm.",
    "crumbs": [
      "Home",
      "Chapters",
      "Chapter 5: An Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "chapter7.html",
    "href": "chapter7.html",
    "title": "Chapter 7: Classification II - OBIA, Subpixel Analysis, and Accuracy Assessment",
    "section": "",
    "text": "Object-Based Image Analysis (OBIA) represents an innovative method for classifying remote sensing imagery, significantly surpassing traditional pixel-based techniques by analyzing images through segmented, meaningful geographic objects. This method integrates spectral data alongside spatial context, shape, texture, and hierarchical information, enhancing accuracy, especially in complex and heterogeneous landscapes. Complementing OBIA, subpixel analysis addresses the mixed-pixel problem common in medium-to-coarse resolution imagery. Techniques such as spectral unmixing quantify fractional abundances of multiple land-cover classes within individual pixels, enabling the detection of components smaller than sensor resolution. Integrating OBIA and subpixel analysis thus yields more detailed and accurate land-cover maps crucial for ecological monitoring and urban planning. \n\nFig 1.1 SegOptim Source:João Gonçalves 2020\n\nAccuracy assessments of these advanced classification methods typically involve constructing error matrices (confusion matrices), summarizing agreement between ground truth and classified results. Key derived accuracy metrics include the Kappa coefficient, Producer’s Accuracy (PA), User’s Accuracy (UA), and Overall Accuracy (OA). The Kappa coefficient adjusts classification accuracy for chance agreement, whereas Producer’s Accuracy indicates the probability of correctly classifying reference data, reflecting omission errors. User’s Accuracy represents reliability from a user’s perspective, highlighting commission errors. Overall Accuracy, an intuitive metric, measures correctly classified pixels across all classes, providing an overall evaluation. \n\nFig 1.2 Source:Barsi et al. 2018 Accuracy Dimensions in Remote Sensing\n\n//------------------ Classification -------------------\n\n// Make a FeatureCollection from the polygons\nvar points = ee.FeatureCollection([\n  ee.Feature(urban, {'class': 1}),\n  ee.Feature(grass, {'class': 2}),\n  ee.Feature(bare_earth, {'class': 5}),\n  ee.Feature(forest, {'class': 6}),\n]);\n\n// The name of the property on the points storing the class label.\nvar classProperty = 'class';\n\n// Sample the composite to generate training data.  Note that the\n// class label is stored in the 'landcover' property.\nvar training = objectPropertiesImage.sampleRegions({\n  collection: points,\n  properties: [classProperty],\n  scale: 30\n});\n\n// Train a CART classifier.\nvar classifier = ee.Classifier.smileCart().train({\n  features: training,\n  classProperty: classProperty,\n});\n\nvar classified = objectPropertiesImage.classify(classifier);\n\nMap.addLayer(classified, {min: 1, max: 5, palette: ['d99282', 'dfdfc2', 'b3ac9f', '1c5f2c']}, \"classified\");\nAdditionally, spatial cross-validation methods enhance accuracy evaluations by explicitly considering spatial autocorrelation within remote sensing datasets. By partitioning data into spatially independent subsets, spatial cross-validation offers robust validation results, improving the reliability and generalizability of OBIA and subpixel analyses in practical scenarios. \n\nFig 1.3 Source: Spatial visualization of selected test and training observations for cross-validation of one repetition. Random (upper row) and spatial partitioning (lower row). Source:Lovelace et al. 2022",
    "crumbs": [
      "Home",
      "Chapters",
      "Chapter 7: Classification II - OBIA, Subpixel Analysis, and Accuracy Assessment"
    ]
  }
]